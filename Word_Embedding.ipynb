{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "\n",
    "pd.options.display.max_colwidth = 200\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "wpt = nltk.WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_document(doc):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    tokens = wpt.tokenize(doc)\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc\n",
    "\n",
    "normalize_corpus = np.vectorize(normalize_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is testing example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['The sky is blue and beautiful.',\n",
    "          'Love this blue and beautiful sky!',\n",
    "          'The quick brown fox jumps over the lazy dog.',\n",
    "          \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
    "          'I love green eggs, ham, sausages and bacon!',\n",
    "          'The brown fox is quick and the blue dog is lazy!',\n",
    "          'The sky is very blue and the sky is very beautiful today',\n",
    "          'The dog is lazy but the brown fox is quick!'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.array(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The sky is blue and beautiful.',\n",
       "       'Love this blue and beautiful sky!',\n",
       "       'The quick brown fox jumps over the lazy dog.',\n",
       "       \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
       "       'I love green eggs, ham, sausages and bacon!',\n",
       "       'The brown fox is quick and the blue dog is lazy!',\n",
       "       'The sky is very blue and the sky is very beautiful today',\n",
       "       'The dog is lazy but the brown fox is quick!'], dtype='<U66')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love green eggs, ham, sausages and bacon!'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = re.sub(r'[^a-zA-Z\\s]', '', corpus[4], re.I|re.A)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = doc.lower()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i love green eggs ham sausages and bacon'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = doc.strip()\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'green', 'eggs', 'ham', 'sausages', 'and', 'bacon']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = wpt.tokenize(doc)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love', 'green', 'eggs', 'ham', 'sausages', 'bacon']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love green eggs ham sausages bacon'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = ' '.join(filtered_tokens)\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sky blue beautiful', 'love blue beautiful sky',\n",
       "       'quick brown fox jumps lazy dog',\n",
       "       'kings breakfast sausages ham bacon eggs toast beans',\n",
       "       'love green eggs ham sausages bacon',\n",
       "       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n",
       "       'dog lazy brown fox quick'], dtype='<U51')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_corpus = normalize_corpus(corpus)\n",
    "norm_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PlaintextCorpusReader in 'C:\\\\Users\\\\vhuang\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\gutenberg'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "bible = gutenberg.sents('bible-kjv.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_terms = punctuation + '0123456789'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[', 'The', 'King', 'James', 'Bible', ']'],\n",
       " ['The', 'Old', 'Testament', 'of', 'the', 'King', 'James', 'Bible'],\n",
       " ['The', 'First', 'Book', 'of', 'Moses', ':', 'Called', 'Genesis']]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [[word.lower() for word in sent if word not in remove_terms] for sent in bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'king', 'james', 'bible'],\n",
       " ['the', 'old', 'testament', 'of', 'the', 'king', 'james', 'bible'],\n",
       " ['the', 'first', 'book', 'of', 'moses', 'called', 'genesis']]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the king james bible',\n",
       " 'the old testament of the king james bible',\n",
       " 'the first book of moses called genesis',\n",
       " 'in the beginning god created the heaven and the earth',\n",
       " 'and the earth was without form and void and darkness was upon the face of the deep']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
    "norm_bible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = filter(None, normalize_corpus(norm_bible))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing word2vec using CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(norm_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shall': 1,\n",
       " 'unto': 2,\n",
       " 'lord': 3,\n",
       " 'thou': 4,\n",
       " 'thy': 5,\n",
       " 'god': 6,\n",
       " 'ye': 7,\n",
       " 'said': 8,\n",
       " 'thee': 9,\n",
       " 'upon': 10,\n",
       " 'man': 11,\n",
       " 'israel': 12,\n",
       " 'king': 13,\n",
       " 'son': 14,\n",
       " 'hath': 15,\n",
       " 'people': 16,\n",
       " 'came': 17,\n",
       " 'house': 18,\n",
       " 'come': 19,\n",
       " 'one': 20,\n",
       " 'children': 21,\n",
       " 'also': 22,\n",
       " 'day': 23,\n",
       " 'land': 24,\n",
       " 'men': 25,\n",
       " 'shalt': 26,\n",
       " 'let': 27,\n",
       " 'go': 28,\n",
       " 'hand': 29,\n",
       " 'saying': 30,\n",
       " 'us': 31,\n",
       " 'made': 32,\n",
       " 'even': 33,\n",
       " 'went': 34,\n",
       " 'behold': 35,\n",
       " 'saith': 36,\n",
       " 'every': 37,\n",
       " 'therefore': 38,\n",
       " 'things': 39,\n",
       " 'father': 40,\n",
       " 'sons': 41,\n",
       " 'hast': 42,\n",
       " 'david': 43,\n",
       " 'make': 44,\n",
       " 'say': 45,\n",
       " 'may': 46,\n",
       " 'earth': 47,\n",
       " 'jesus': 48,\n",
       " 'great': 49,\n",
       " 'name': 50,\n",
       " 'thine': 51,\n",
       " 'away': 52,\n",
       " 'put': 53,\n",
       " 'among': 54,\n",
       " 'thereof': 55,\n",
       " 'forth': 56,\n",
       " 'give': 57,\n",
       " 'neither': 58,\n",
       " 'take': 59,\n",
       " 'city': 60,\n",
       " 'days': 61,\n",
       " 'brought': 62,\n",
       " 'moses': 63,\n",
       " 'two': 64,\n",
       " 'heart': 65,\n",
       " 'pass': 66,\n",
       " 'judah': 67,\n",
       " 'jerusalem': 68,\n",
       " 'according': 69,\n",
       " 'know': 70,\n",
       " 'took': 71,\n",
       " 'thus': 72,\n",
       " 'offering': 73,\n",
       " 'bring': 74,\n",
       " 'good': 75,\n",
       " 'place': 76,\n",
       " 'word': 77,\n",
       " 'set': 78,\n",
       " 'sent': 79,\n",
       " 'yet': 80,\n",
       " 'like': 81,\n",
       " 'way': 82,\n",
       " 'eat': 83,\n",
       " 'mine': 84,\n",
       " 'heard': 85,\n",
       " 'called': 86,\n",
       " 'time': 87,\n",
       " 'evil': 88,\n",
       " 'holy': 89,\n",
       " 'egypt': 90,\n",
       " 'see': 91,\n",
       " 'hundred': 92,\n",
       " 'spake': 93,\n",
       " 'heaven': 94,\n",
       " 'christ': 95,\n",
       " 'done': 96,\n",
       " 'brethren': 97,\n",
       " 'many': 98,\n",
       " 'hear': 99,\n",
       " 'fire': 100,\n",
       " 'saw': 101,\n",
       " 'fathers': 102,\n",
       " 'words': 103,\n",
       " 'priest': 104,\n",
       " 'thing': 105,\n",
       " 'years': 106,\n",
       " 'law': 107,\n",
       " 'thousand': 108,\n",
       " 'speak': 109,\n",
       " 'voice': 110,\n",
       " 'spirit': 111,\n",
       " 'eyes': 112,\n",
       " 'cast': 113,\n",
       " 'given': 114,\n",
       " 'servant': 115,\n",
       " 'art': 116,\n",
       " 'together': 117,\n",
       " 'three': 118,\n",
       " 'servants': 119,\n",
       " 'answered': 120,\n",
       " 'ever': 121,\n",
       " 'might': 122,\n",
       " 'gave': 123,\n",
       " 'hands': 124,\n",
       " 'soul': 125,\n",
       " 'seven': 126,\n",
       " 'would': 127,\n",
       " 'another': 128,\n",
       " 'life': 129,\n",
       " 'cities': 130,\n",
       " 'blood': 131,\n",
       " 'first': 132,\n",
       " 'sin': 133,\n",
       " 'commanded': 134,\n",
       " 'side': 135,\n",
       " 'without': 136,\n",
       " 'sword': 137,\n",
       " 'peace': 138,\n",
       " 'mouth': 139,\n",
       " 'saul': 140,\n",
       " 'flesh': 141,\n",
       " 'work': 142,\n",
       " 'gold': 143,\n",
       " 'face': 144,\n",
       " 'high': 145,\n",
       " 'wife': 146,\n",
       " 'found': 147,\n",
       " 'brother': 148,\n",
       " 'sea': 149,\n",
       " 'priests': 150,\n",
       " 'fear': 151,\n",
       " 'glory': 152,\n",
       " 'water': 153,\n",
       " 'old': 154,\n",
       " 'altar': 155,\n",
       " 'jacob': 156,\n",
       " 'death': 157,\n",
       " 'year': 158,\n",
       " 'drink': 159,\n",
       " 'burnt': 160,\n",
       " 'midst': 161,\n",
       " 'woman': 162,\n",
       " 'head': 163,\n",
       " 'congregation': 164,\n",
       " 'keep': 165,\n",
       " 'dead': 166,\n",
       " 'bread': 167,\n",
       " 'right': 168,\n",
       " 'none': 169,\n",
       " 'aaron': 170,\n",
       " 'left': 171,\n",
       " 'toward': 172,\n",
       " 'five': 173,\n",
       " 'wherefore': 174,\n",
       " 'wicked': 175,\n",
       " 'kingdom': 176,\n",
       " 'kings': 177,\n",
       " 'yea': 178,\n",
       " 'dwell': 179,\n",
       " 'stood': 180,\n",
       " 'taken': 181,\n",
       " 'nations': 182,\n",
       " 'sight': 183,\n",
       " 'four': 184,\n",
       " 'tabernacle': 185,\n",
       " 'cause': 186,\n",
       " 'daughter': 187,\n",
       " 'silver': 188,\n",
       " 'round': 189,\n",
       " 'die': 190,\n",
       " 'mother': 191,\n",
       " 'cut': 192,\n",
       " 'whose': 193,\n",
       " 'pray': 194,\n",
       " 'love': 195,\n",
       " 'righteousness': 196,\n",
       " 'night': 197,\n",
       " 'end': 198,\n",
       " 'wilderness': 199,\n",
       " 'solomon': 200,\n",
       " 'blessed': 201,\n",
       " 'young': 202,\n",
       " 'hosts': 203,\n",
       " 'deliver': 204,\n",
       " 'twenty': 205,\n",
       " 'judgment': 206,\n",
       " 'babylon': 207,\n",
       " 'covenant': 208,\n",
       " 'field': 209,\n",
       " 'meat': 210,\n",
       " 'delivered': 211,\n",
       " 'waters': 212,\n",
       " 'turned': 213,\n",
       " 'much': 214,\n",
       " 'world': 215,\n",
       " 'surely': 216,\n",
       " 'spoken': 217,\n",
       " 'mighty': 218,\n",
       " 'turn': 219,\n",
       " 'chief': 220,\n",
       " 'cometh': 221,\n",
       " 'told': 222,\n",
       " 'seed': 223,\n",
       " 'laid': 224,\n",
       " 'seen': 225,\n",
       " 'written': 226,\n",
       " 'mercy': 227,\n",
       " 'gate': 228,\n",
       " 'rest': 229,\n",
       " 'iniquity': 230,\n",
       " 'princes': 231,\n",
       " 'light': 232,\n",
       " 'pharaoh': 233,\n",
       " 'stand': 234,\n",
       " 'power': 235,\n",
       " 'enemies': 236,\n",
       " 'gathered': 237,\n",
       " 'levites': 238,\n",
       " 'offerings': 239,\n",
       " 'return': 240,\n",
       " 'mount': 241,\n",
       " 'well': 242,\n",
       " 'destroy': 243,\n",
       " 'full': 244,\n",
       " 'feet': 245,\n",
       " 'strong': 246,\n",
       " 'jews': 247,\n",
       " 'philistines': 248,\n",
       " 'fall': 249,\n",
       " 'daughters': 250,\n",
       " 'whole': 251,\n",
       " 'month': 252,\n",
       " 'joseph': 253,\n",
       " 'ten': 254,\n",
       " 'abraham': 255,\n",
       " 'live': 256,\n",
       " 'wilt': 257,\n",
       " 'faith': 258,\n",
       " 'wise': 259,\n",
       " 'gods': 260,\n",
       " 'fell': 261,\n",
       " 'multitude': 262,\n",
       " 'prophet': 263,\n",
       " 'praise': 264,\n",
       " 'disciples': 265,\n",
       " 'little': 266,\n",
       " 'seek': 267,\n",
       " 'tribe': 268,\n",
       " 'lest': 269,\n",
       " 'strength': 270,\n",
       " 'concerning': 271,\n",
       " 'lay': 272,\n",
       " 'inheritance': 273,\n",
       " 'prophets': 274,\n",
       " 'righteous': 275,\n",
       " 'truth': 276,\n",
       " 'offer': 277,\n",
       " 'works': 278,\n",
       " 'send': 279,\n",
       " 'anger': 280,\n",
       " 'though': 281,\n",
       " 'wine': 282,\n",
       " 'save': 283,\n",
       " 'wisdom': 284,\n",
       " 'ark': 285,\n",
       " 'therein': 286,\n",
       " 'morning': 287,\n",
       " 'shew': 288,\n",
       " 'smote': 289,\n",
       " 'dwelt': 290,\n",
       " 'begat': 291,\n",
       " 'war': 292,\n",
       " 'nothing': 293,\n",
       " 'known': 294,\n",
       " 'sacrifice': 295,\n",
       " 'bear': 296,\n",
       " 'tell': 297,\n",
       " 'places': 298,\n",
       " 'joshua': 299,\n",
       " 'thyself': 300,\n",
       " 'cubits': 301,\n",
       " 'gone': 302,\n",
       " 'part': 303,\n",
       " 'walk': 304,\n",
       " 'departed': 305,\n",
       " 'near': 306,\n",
       " 'long': 307,\n",
       " 'fruit': 308,\n",
       " 'serve': 309,\n",
       " 'book': 310,\n",
       " 'doth': 311,\n",
       " 'poor': 312,\n",
       " 'temple': 313,\n",
       " 'ways': 314,\n",
       " 'six': 315,\n",
       " 'child': 316,\n",
       " 'inhabitants': 317,\n",
       " 'tree': 318,\n",
       " 'angel': 319,\n",
       " 'oil': 320,\n",
       " 'died': 321,\n",
       " 'cried': 322,\n",
       " 'wrath': 323,\n",
       " 'jordan': 324,\n",
       " 'manner': 325,\n",
       " 'certain': 326,\n",
       " 'slew': 327,\n",
       " 'call': 328,\n",
       " 'unclean': 329,\n",
       " 'afraid': 330,\n",
       " 'rejoice': 331,\n",
       " 'host': 332,\n",
       " 'ground': 333,\n",
       " 'stone': 334,\n",
       " 'judge': 335,\n",
       " 'sat': 336,\n",
       " 'door': 337,\n",
       " 'sheep': 338,\n",
       " 'twelve': 339,\n",
       " 'within': 340,\n",
       " 'broken': 341,\n",
       " 'hold': 342,\n",
       " 'third': 343,\n",
       " 'bare': 344,\n",
       " 'slain': 345,\n",
       " 'cannot': 346,\n",
       " 'whosoever': 347,\n",
       " 'returned': 348,\n",
       " 'women': 349,\n",
       " 'beast': 350,\n",
       " 'cry': 351,\n",
       " 'master': 352,\n",
       " 'river': 353,\n",
       " 'wall': 354,\n",
       " 'elders': 355,\n",
       " 'began': 356,\n",
       " 'mountains': 357,\n",
       " 'country': 358,\n",
       " 'stones': 359,\n",
       " 'number': 360,\n",
       " 'ephraim': 361,\n",
       " 'commandment': 362,\n",
       " 'second': 363,\n",
       " 'reigned': 364,\n",
       " 'throne': 365,\n",
       " 'receive': 366,\n",
       " 'kept': 367,\n",
       " 'paul': 368,\n",
       " 'thirty': 369,\n",
       " 'body': 370,\n",
       " 'families': 371,\n",
       " 'far': 372,\n",
       " 'sins': 373,\n",
       " 'knowledge': 374,\n",
       " 'whether': 375,\n",
       " 'commandments': 376,\n",
       " 'battle': 377,\n",
       " 'moreover': 378,\n",
       " 'till': 379,\n",
       " 'knew': 380,\n",
       " 'built': 381,\n",
       " 'arose': 382,\n",
       " 'reign': 383,\n",
       " 'wherein': 384,\n",
       " 'destroyed': 385,\n",
       " 'moab': 386,\n",
       " 'could': 387,\n",
       " 'gather': 388,\n",
       " 'benjamin': 389,\n",
       " 'joy': 390,\n",
       " 'grace': 391,\n",
       " 'peter': 392,\n",
       " 'build': 393,\n",
       " 'salvation': 394,\n",
       " 'throughout': 395,\n",
       " 'darkness': 396,\n",
       " 'passed': 397,\n",
       " 'able': 398,\n",
       " 'sun': 399,\n",
       " 'received': 400,\n",
       " 'born': 401,\n",
       " 'forty': 402,\n",
       " 'lo': 403,\n",
       " 'border': 404,\n",
       " 'trees': 405,\n",
       " 'filled': 406,\n",
       " 'fifty': 407,\n",
       " 'understanding': 408,\n",
       " 'east': 409,\n",
       " 'lifted': 410,\n",
       " 'find': 411,\n",
       " 'beasts': 412,\n",
       " 'look': 413,\n",
       " 'lie': 414,\n",
       " 'vessels': 415,\n",
       " 'cattle': 416,\n",
       " 'back': 417,\n",
       " 'zion': 418,\n",
       " 'whatsoever': 419,\n",
       " 'hearken': 420,\n",
       " 'ears': 421,\n",
       " 'new': 422,\n",
       " 'heathen': 423,\n",
       " 'enter': 424,\n",
       " 'arise': 425,\n",
       " 'desolate': 426,\n",
       " 'manasseh': 427,\n",
       " 'jeremiah': 428,\n",
       " 'living': 429,\n",
       " 'remember': 430,\n",
       " 'thence': 431,\n",
       " 'sake': 432,\n",
       " 'nation': 433,\n",
       " 'times': 434,\n",
       " 'carried': 435,\n",
       " 'joab': 436,\n",
       " 'fled': 437,\n",
       " 'gates': 438,\n",
       " 'samuel': 439,\n",
       " 'looked': 440,\n",
       " 'offered': 441,\n",
       " 'south': 442,\n",
       " 'honour': 443,\n",
       " 'counsel': 444,\n",
       " 'believe': 445,\n",
       " 'wood': 446,\n",
       " 'rise': 447,\n",
       " 'break': 448,\n",
       " 'verily': 449,\n",
       " 'valley': 450,\n",
       " 'money': 451,\n",
       " 'captain': 452,\n",
       " 'john': 453,\n",
       " 'burn': 454,\n",
       " 'sanctuary': 455,\n",
       " 'opened': 456,\n",
       " 'mountain': 457,\n",
       " 'half': 458,\n",
       " 'houses': 459,\n",
       " 'camp': 460,\n",
       " 'sabbath': 461,\n",
       " 'goeth': 462,\n",
       " 'neighbour': 463,\n",
       " 'become': 464,\n",
       " 'shewed': 465,\n",
       " 'heavens': 466,\n",
       " 'wives': 467,\n",
       " 'trust': 468,\n",
       " 'clean': 469,\n",
       " 'must': 470,\n",
       " 'isaac': 471,\n",
       " 'statutes': 472,\n",
       " 'service': 473,\n",
       " 'meet': 474,\n",
       " 'rose': 475,\n",
       " 'stranger': 476,\n",
       " 'witness': 477,\n",
       " 'beside': 478,\n",
       " 'north': 479,\n",
       " 'fat': 480,\n",
       " 'answer': 481,\n",
       " 'hid': 482,\n",
       " 'incense': 483,\n",
       " 'gentiles': 484,\n",
       " 'tongue': 485,\n",
       " 'numbered': 486,\n",
       " 'hezekiah': 487,\n",
       " 'bless': 488,\n",
       " 'judgments': 489,\n",
       " 'husband': 490,\n",
       " 'wickedness': 491,\n",
       " 'sought': 492,\n",
       " 'maketh': 493,\n",
       " 'captivity': 494,\n",
       " 'help': 495,\n",
       " 'kill': 496,\n",
       " 'brass': 497,\n",
       " 'appointed': 498,\n",
       " 'depart': 499,\n",
       " 'giveth': 500,\n",
       " 'hope': 501,\n",
       " 'open': 502,\n",
       " 'smite': 503,\n",
       " 'whither': 504,\n",
       " 'wind': 505,\n",
       " 'feast': 506,\n",
       " 'family': 507,\n",
       " 'ashamed': 508,\n",
       " 'didst': 509,\n",
       " 'court': 510,\n",
       " 'pieces': 511,\n",
       " 'chosen': 512,\n",
       " 'samaria': 513,\n",
       " 'seventh': 514,\n",
       " 'perish': 515,\n",
       " 'ear': 516,\n",
       " 'jonathan': 517,\n",
       " 'walked': 518,\n",
       " 'lips': 519,\n",
       " 'captains': 520,\n",
       " 'sing': 521,\n",
       " 'rock': 522,\n",
       " 'assyria': 523,\n",
       " 'get': 524,\n",
       " 'asked': 525,\n",
       " 'sinned': 526,\n",
       " 'generations': 527,\n",
       " 'slay': 528,\n",
       " 'spoil': 529,\n",
       " 'presence': 530,\n",
       " 'seeing': 531,\n",
       " 'better': 532,\n",
       " 'eye': 533,\n",
       " 'leave': 534,\n",
       " 'sister': 535,\n",
       " 'believed': 536,\n",
       " 'suburbs': 537,\n",
       " 'mayest': 538,\n",
       " 'beloved': 539,\n",
       " 'lot': 540,\n",
       " 'hearts': 541,\n",
       " 'fine': 542,\n",
       " 'firstborn': 543,\n",
       " 'chariots': 544,\n",
       " 'tribes': 545,\n",
       " 'flock': 546,\n",
       " 'sit': 547,\n",
       " 'horses': 548,\n",
       " 'vain': 549,\n",
       " 'desire': 550,\n",
       " 'wherewith': 551,\n",
       " 'enemy': 552,\n",
       " 'heads': 553,\n",
       " 'spread': 554,\n",
       " 'ghost': 555,\n",
       " 'ask': 556,\n",
       " 'trouble': 557,\n",
       " 'prayer': 558,\n",
       " 'dust': 559,\n",
       " 'alone': 560,\n",
       " 'sweet': 561,\n",
       " 'worship': 562,\n",
       " 'teach': 563,\n",
       " 'generation': 564,\n",
       " 'entered': 565,\n",
       " 'cloud': 566,\n",
       " 'lamb': 567,\n",
       " 'fight': 568,\n",
       " 'bullock': 569,\n",
       " 'absalom': 570,\n",
       " 'gospel': 571,\n",
       " 'became': 572,\n",
       " 'buried': 573,\n",
       " 'possess': 574,\n",
       " 'present': 575,\n",
       " 'likewise': 576,\n",
       " 'eaten': 577,\n",
       " 'shut': 578,\n",
       " 'covered': 579,\n",
       " 'prince': 580,\n",
       " 'wait': 581,\n",
       " 'beginning': 582,\n",
       " 'lift': 583,\n",
       " 'flee': 584,\n",
       " 'possession': 585,\n",
       " 'command': 586,\n",
       " 'bound': 587,\n",
       " 'followed': 588,\n",
       " 'linen': 589,\n",
       " 'saved': 590,\n",
       " 'jeroboam': 591,\n",
       " 'garments': 592,\n",
       " 'lion': 593,\n",
       " 'rain': 594,\n",
       " 'charge': 595,\n",
       " 'corn': 596,\n",
       " 'knoweth': 597,\n",
       " 'image': 598,\n",
       " 'iron': 599,\n",
       " 'curse': 600,\n",
       " 'oxen': 601,\n",
       " 'morrow': 602,\n",
       " 'prepared': 603,\n",
       " 'clothes': 604,\n",
       " 'utterly': 605,\n",
       " 'idols': 606,\n",
       " 'bow': 607,\n",
       " 'still': 608,\n",
       " 'ready': 609,\n",
       " 'coming': 610,\n",
       " 'esau': 611,\n",
       " 'gilead': 612,\n",
       " 'wrought': 613,\n",
       " 'nigh': 614,\n",
       " 'minister': 615,\n",
       " 'bones': 616,\n",
       " 'perfect': 617,\n",
       " 'portion': 618,\n",
       " 'shame': 619,\n",
       " 'woe': 620,\n",
       " 'tent': 621,\n",
       " 'ram': 622,\n",
       " 'sore': 623,\n",
       " 'loved': 624,\n",
       " 'riches': 625,\n",
       " 'burned': 626,\n",
       " 'plague': 627,\n",
       " 'anointed': 628,\n",
       " 'everlasting': 629,\n",
       " 'egyptians': 630,\n",
       " 'ought': 631,\n",
       " 'goats': 632,\n",
       " 'pure': 633,\n",
       " 'names': 634,\n",
       " 'liveth': 635,\n",
       " 'famine': 636,\n",
       " 'consumed': 637,\n",
       " 'shekels': 638,\n",
       " 'doeth': 639,\n",
       " 'suffer': 640,\n",
       " 'nevertheless': 641,\n",
       " 'foot': 642,\n",
       " 'small': 643,\n",
       " 'thither': 644,\n",
       " 'declare': 645,\n",
       " 'saints': 646,\n",
       " 'caused': 647,\n",
       " 'angels': 648,\n",
       " 'stead': 649,\n",
       " 'mind': 650,\n",
       " 'threescore': 651,\n",
       " 'destruction': 652,\n",
       " 'ahab': 653,\n",
       " 'hour': 654,\n",
       " 'going': 655,\n",
       " 'committed': 656,\n",
       " 'remnant': 657,\n",
       " 'removed': 658,\n",
       " 'canaan': 659,\n",
       " 'top': 660,\n",
       " 'ammon': 661,\n",
       " 'bed': 662,\n",
       " 'write': 663,\n",
       " 'understand': 664,\n",
       " 'ass': 665,\n",
       " 'carry': 666,\n",
       " 'prison': 667,\n",
       " 'prophesy': 668,\n",
       " 'breadth': 669,\n",
       " 'wash': 670,\n",
       " 'knowest': 671,\n",
       " 'glad': 672,\n",
       " 'pillars': 673,\n",
       " 'sound': 674,\n",
       " 'reproach': 675,\n",
       " 'labour': 676,\n",
       " 'pit': 677,\n",
       " 'increase': 678,\n",
       " 'noise': 679,\n",
       " 'greatly': 680,\n",
       " 'alive': 681,\n",
       " 'hate': 682,\n",
       " 'sick': 683,\n",
       " 'fourth': 684,\n",
       " 'garment': 685,\n",
       " 'early': 686,\n",
       " 'company': 687,\n",
       " 'never': 688,\n",
       " 'ruler': 689,\n",
       " 'pharisees': 690,\n",
       " 'follow': 691,\n",
       " 'last': 692,\n",
       " 'rod': 693,\n",
       " 'raised': 694,\n",
       " 'drew': 695,\n",
       " 'poured': 696,\n",
       " 'fast': 697,\n",
       " 'edom': 698,\n",
       " 'jehoshaphat': 699,\n",
       " 'vanity': 700,\n",
       " 'daniel': 701,\n",
       " 'sleep': 702,\n",
       " 'pitched': 703,\n",
       " 'hide': 704,\n",
       " 'fulfilled': 705,\n",
       " 'army': 706,\n",
       " 'trespass': 707,\n",
       " 'faithful': 708,\n",
       " 'hearkened': 709,\n",
       " 'continually': 710,\n",
       " 'tenth': 711,\n",
       " 'rich': 712,\n",
       " 'abide': 713,\n",
       " 'lambs': 714,\n",
       " 'feed': 715,\n",
       " 'sold': 716,\n",
       " 'blind': 717,\n",
       " 'prepare': 718,\n",
       " 'atonement': 719,\n",
       " 'taught': 720,\n",
       " 'eight': 721,\n",
       " 'abroad': 722,\n",
       " 'flocks': 723,\n",
       " 'thought': 724,\n",
       " 'matter': 725,\n",
       " 'heed': 726,\n",
       " 'true': 727,\n",
       " 'fallen': 728,\n",
       " 'reward': 729,\n",
       " 'healed': 730,\n",
       " 'messengers': 731,\n",
       " 'remain': 732,\n",
       " 'sacrifices': 733,\n",
       " 'bringeth': 734,\n",
       " 'souls': 735,\n",
       " 'vision': 736,\n",
       " 'bowed': 737,\n",
       " 'strangers': 738,\n",
       " 'rulers': 739,\n",
       " 'greater': 740,\n",
       " 'length': 741,\n",
       " 'rams': 742,\n",
       " 'ones': 743,\n",
       " 'rivers': 744,\n",
       " 'hill': 745,\n",
       " 'skin': 746,\n",
       " 'church': 747,\n",
       " 'sware': 748,\n",
       " 'draw': 749,\n",
       " 'precious': 750,\n",
       " 'strange': 751,\n",
       " 'abomination': 752,\n",
       " 'sign': 753,\n",
       " 'passover': 754,\n",
       " 'testimony': 755,\n",
       " 'wings': 756,\n",
       " 'abominations': 757,\n",
       " 'forsaken': 758,\n",
       " 'moved': 759,\n",
       " 'plain': 760,\n",
       " 'affliction': 761,\n",
       " 'white': 762,\n",
       " 'branches': 763,\n",
       " 'villages': 764,\n",
       " 'commit': 765,\n",
       " 'syria': 766,\n",
       " 'simon': 767,\n",
       " 'established': 768,\n",
       " 'served': 769,\n",
       " 'behind': 770,\n",
       " 'feared': 771,\n",
       " 'dream': 772,\n",
       " 'reuben': 773,\n",
       " 'except': 774,\n",
       " 'speaketh': 775,\n",
       " 'eleazar': 776,\n",
       " 'taketh': 777,\n",
       " 'shouldest': 778,\n",
       " 'clothed': 779,\n",
       " 'faces': 780,\n",
       " 'hebron': 781,\n",
       " 'amorites': 782,\n",
       " 'shadow': 783,\n",
       " 'prey': 784,\n",
       " 'brake': 785,\n",
       " 'table': 786,\n",
       " 'thanks': 787,\n",
       " 'whereof': 788,\n",
       " 'cursed': 789,\n",
       " 'dan': 790,\n",
       " 'levi': 791,\n",
       " 'gad': 792,\n",
       " 'aside': 793,\n",
       " 'cover': 794,\n",
       " 'galilee': 795,\n",
       " 'dry': 796,\n",
       " 'whence': 797,\n",
       " 'scattered': 798,\n",
       " 'stretched': 799,\n",
       " 'images': 800,\n",
       " 'defiled': 801,\n",
       " 'reason': 802,\n",
       " 'run': 803,\n",
       " 'devour': 804,\n",
       " 'lebanon': 805,\n",
       " 'doors': 806,\n",
       " 'ship': 807,\n",
       " 'youth': 808,\n",
       " 'cease': 809,\n",
       " 'appeared': 810,\n",
       " 'indeed': 811,\n",
       " 'favour': 812,\n",
       " 'since': 813,\n",
       " 'sanctify': 814,\n",
       " 'read': 815,\n",
       " 'fool': 816,\n",
       " 'sorrow': 817,\n",
       " 'vineyard': 818,\n",
       " 'wept': 819,\n",
       " 'womb': 820,\n",
       " 'obey': 821,\n",
       " 'fury': 822,\n",
       " 'abode': 823,\n",
       " 'measure': 824,\n",
       " 'others': 825,\n",
       " 'divided': 826,\n",
       " 'west': 827,\n",
       " 'worshipped': 828,\n",
       " 'worthy': 829,\n",
       " 'grave': 830,\n",
       " 'upright': 831,\n",
       " 'cup': 832,\n",
       " 'secret': 833,\n",
       " 'abundance': 834,\n",
       " 'scribes': 835,\n",
       " 'elijah': 836,\n",
       " 'blessing': 837,\n",
       " 'horns': 838,\n",
       " 'led': 839,\n",
       " 'troubled': 840,\n",
       " 'killed': 841,\n",
       " 'beseech': 842,\n",
       " 'arm': 843,\n",
       " 'consider': 844,\n",
       " 'rule': 845,\n",
       " 'wast': 846,\n",
       " 'tents': 847,\n",
       " 'afterward': 848,\n",
       " 'hither': 849,\n",
       " 'abimelech': 850,\n",
       " 'golden': 851,\n",
       " 'kindled': 852,\n",
       " 'thereon': 853,\n",
       " 'rent': 854,\n",
       " 'crown': 855,\n",
       " 'walls': 856,\n",
       " 'acts': 857,\n",
       " 'chambers': 858,\n",
       " 'chaldeans': 859,\n",
       " 'deep': 860,\n",
       " 'comfort': 861,\n",
       " 'hills': 862,\n",
       " 'bethel': 863,\n",
       " 'soon': 864,\n",
       " 'prayed': 865,\n",
       " 'loveth': 866,\n",
       " 'streets': 867,\n",
       " 'cherubims': 868,\n",
       " 'asses': 869,\n",
       " 'shechem': 870,\n",
       " 'chariot': 871,\n",
       " 'fail': 872,\n",
       " 'parts': 873,\n",
       " 'fought': 874,\n",
       " 'false': 875,\n",
       " 'ox': 876,\n",
       " 'hair': 877,\n",
       " 'waste': 878,\n",
       " 'baal': 879,\n",
       " 'exalted': 880,\n",
       " 'howbeit': 881,\n",
       " 'judged': 882,\n",
       " 'loins': 883,\n",
       " 'pour': 884,\n",
       " 'daily': 885,\n",
       " 'smitten': 886,\n",
       " 'burden': 887,\n",
       " 'jericho': 888,\n",
       " 'balaam': 889,\n",
       " 'abner': 890,\n",
       " 'grass': 891,\n",
       " 'fifth': 892,\n",
       " 'dominion': 893,\n",
       " 'sanctified': 894,\n",
       " 'hurt': 895,\n",
       " 'always': 896,\n",
       " 'height': 897,\n",
       " 'inherit': 898,\n",
       " 'thousands': 899,\n",
       " 'neck': 900,\n",
       " 'pleased': 901,\n",
       " 'vine': 902,\n",
       " 'think': 903,\n",
       " 'redeemed': 904,\n",
       " 'coast': 905,\n",
       " 'blemish': 906,\n",
       " 'wrote': 907,\n",
       " 'treasures': 908,\n",
       " 'rather': 909,\n",
       " 'zedekiah': 910,\n",
       " 'seventy': 911,\n",
       " 'upward': 912,\n",
       " 'harvest': 913,\n",
       " 'abram': 914,\n",
       " 'ran': 915,\n",
       " 'pleasure': 916,\n",
       " 'household': 917,\n",
       " 'unleavened': 918,\n",
       " 'order': 919,\n",
       " 'oath': 920,\n",
       " 'speaking': 921,\n",
       " 'watch': 922,\n",
       " 'syrians': 923,\n",
       " 'preached': 924,\n",
       " 'devil': 925,\n",
       " 'evening': 926,\n",
       " 'burning': 927,\n",
       " 'despised': 928,\n",
       " 'next': 929,\n",
       " 'deal': 930,\n",
       " 'swear': 931,\n",
       " 'journey': 932,\n",
       " 'hated': 933,\n",
       " 'loud': 934,\n",
       " 'governor': 935,\n",
       " 'job': 936,\n",
       " 'trumpet': 937,\n",
       " 'asa': 938,\n",
       " 'nebuchadnezzar': 939,\n",
       " 'mordecai': 940,\n",
       " 'baptized': 941,\n",
       " 'lieth': 942,\n",
       " 'captive': 943,\n",
       " 'escape': 944,\n",
       " 'yoke': 945,\n",
       " 'gift': 946,\n",
       " 'raise': 947,\n",
       " 'months': 948,\n",
       " 'horsemen': 949,\n",
       " 'fields': 950,\n",
       " 'choose': 951,\n",
       " 'bashan': 952,\n",
       " 'jehu': 953,\n",
       " 'endureth': 954,\n",
       " 'apostles': 955,\n",
       " 'lived': 956,\n",
       " 'escaped': 957,\n",
       " 'weight': 958,\n",
       " 'lead': 959,\n",
       " 'officers': 960,\n",
       " 'habitation': 961,\n",
       " 'free': 962,\n",
       " 'flour': 963,\n",
       " 'forsake': 964,\n",
       " 'elisha': 965,\n",
       " 'thoughts': 966,\n",
       " 'violence': 967,\n",
       " 'remembered': 968,\n",
       " 'nakedness': 969,\n",
       " 'damascus': 970,\n",
       " 'exceeding': 971,\n",
       " 'dealt': 972,\n",
       " 'named': 973,\n",
       " 'raiment': 974,\n",
       " 'lying': 975,\n",
       " 'drive': 976,\n",
       " 'seat': 977,\n",
       " 'dwelleth': 978,\n",
       " 'kingdoms': 979,\n",
       " 'pleasant': 980,\n",
       " 'slaughter': 981,\n",
       " 'persons': 982,\n",
       " 'edge': 983,\n",
       " 'person': 984,\n",
       " 'season': 985,\n",
       " 'consume': 986,\n",
       " 'buy': 987,\n",
       " 'honey': 988,\n",
       " 'eateth': 989,\n",
       " 'tables': 990,\n",
       " 'iniquities': 991,\n",
       " 'satan': 992,\n",
       " 'esther': 993,\n",
       " 'pilate': 994,\n",
       " 'food': 995,\n",
       " 'fowls': 996,\n",
       " 'canaanites': 997,\n",
       " 'countries': 998,\n",
       " 'almighty': 999,\n",
       " 'haste': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2id = tokenizer.word_index\n",
    "word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PAD'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 1154, 5766],\n",
       " [154, 2450, 13, 1154, 5766],\n",
       " [132, 310, 63, 86, 8480],\n",
       " [582, 6, 1180, 94, 47],\n",
       " [47, 136, 1883, 1884, 396, 10, 144, 860]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grace lord jesus christ'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grace', 'lord', 'jesus', 'christ']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text_to_word_sequence(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sentence_length = len(words)\n",
    "        for index, word in enumerate(words):\n",
    "            context_words = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            \n",
    "            context_words.append([words[i]\n",
    "                                 for i in range(start, end)\n",
    "                                 if 0 <= i < sentence_length\n",
    "                                 and i != index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(context_words, maxlen=context_length)\n",
    "            y = np_utils.to_categorical(label_word, vocab_size)\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (X): ['old', 'testament', 'james', 'bible'] -> Target (Y): king\n",
      "Context (X): ['first', 'book', 'called', 'genesis'] -> Target (Y): moses\n",
      "Context (X): ['beginning', 'god', 'heaven', 'earth'] -> Target (Y): created\n",
      "Context (X): ['earth', 'without', 'void', 'darkness'] -> Target (Y): form\n",
      "Context (X): ['without', 'form', 'darkness', 'upon'] -> Target (Y): void\n",
      "Context (X): ['form', 'void', 'upon', 'face'] -> Target (Y): darkness\n",
      "Context (X): ['void', 'darkness', 'face', 'deep'] -> Target (Y): upon\n",
      "Context (X): ['spirit', 'god', 'upon', 'face'] -> Target (Y): moved\n",
      "Context (X): ['god', 'moved', 'face', 'waters'] -> Target (Y): upon\n",
      "Context (X): ['god', 'said', 'light', 'light'] -> Target (Y): let\n",
      "Context (X): ['god', 'saw', 'good', 'god'] -> Target (Y): light\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size,\n",
    "                                        vocab_size=vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
    "    \n",
    "        if i == 10:\n",
    "            break\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CBOW Deep Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Sequential()\n",
    "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 100)            1242500   \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12425)             1254925   \n",
      "=================================================================\n",
      "Total params: 2,497,425\n",
      "Trainable params: 2,497,425\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100000 (context, word) pairs\n",
      "Processed 200000 (context, word) pairs\n",
      "Processed 300000 (context, word) pairs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-185-e7856a992586>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m  \u001b[1;32min\u001b[0m \u001b[0mgenerate_context_word_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Processed {} (context, word) pairs'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1254\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3215\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3217\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3218\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n\u001b[0;32m   3219\u001b[0m                                  [x.numpy() for x in outputs])\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    556\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m    557\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m--> 558\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[1;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 627\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    628\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args)\u001b[0m\n\u001b[0;32m    413\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[0;32m    414\u001b[0m                    \"config_proto\", config),\n\u001b[1;32m--> 415\u001b[1;33m             ctx=ctx)\n\u001b[0m\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Replace empty list with None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     59\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    i = 0\n",
    "    for x, y  in generate_context_word_pairs(wids, window_size, vocab_size):\n",
    "        i += 1\n",
    "        loss += cbow.train_on_batch(x, y)\n",
    "        if i % 100000 == 0:\n",
    "            print('Processed {} (context, word) pairs'.format(i))\n",
    "\n",
    "    print('Epoch:', epoch, '\\tLoss:', loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12425, 100)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 12425)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12425,)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow.get_weights()[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement word2vec using Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(norm_bible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['king james bible',\n",
       " 'old testament king james bible',\n",
       " 'first book moses called genesis',\n",
       " 'beginning god created heaven earth',\n",
       " 'earth without form void darkness upon face deep']"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_bible[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = {v:k for k, v in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2id) + 1\n",
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate skip-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(skip_grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build  the skip-grams model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Sequential()\n",
    "word_model.add(Input(shape=(embed_size, )))\n",
    "word_model.add(Embedding(vocab_size, embed_size, \n",
    "                         embeddings_initializer='glorot_uniform', \n",
    "                         input_length=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.add(Reshape((embed_size, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model = Sequential()\n",
    "context_model.add(Input(shape=(embed_size, )))\n",
    "context_model.add(Embedding(vocab_size, embed_size, \n",
    "                            embeddings_initializer='glorot_uniform',\n",
    "                            input_length=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model.add(Reshape((embed_size, )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Concatenate([word_model, context_model]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1, kernel_initializer='glorot_uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mse', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-249-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\py36tf2\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[0;32m   1575\u001b[0m     \"\"\"\n\u001b[0;32m   1576\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1577\u001b[1;33m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[0;32m   1578\u001b[0m                        \u001b[1;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1579\u001b[0m                        \u001b[1;34m'`fit()` with some data, or specify '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Input(shape=(embed_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for i, elem in enumerate(skip_grams[:2]):\n",
    "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "\n",
    "        X = [pair_first_elem, pair_second_elem]\n",
    "        Y = labels\n",
    "        if i % 10000 == 0:\n",
    "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\n",
    "        loss += model.train_on_batch(X,Y)  \n",
    "\n",
    "    print('Epoch:', epoch, 'Loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
